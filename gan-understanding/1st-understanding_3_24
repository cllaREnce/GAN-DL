期望值：用minibatch来进行表达，一组一组不光是为了顾全大局的考虑，而是为了体现概率的问题
概率分布：一个东西，可能存在的多少种样子，这个是一个分布; 这个分布不是图片内部的一种分布
为了产生一个假的东西，对抗性网络通过，两种方式来进学习，某个特定object的分布--> 1.我学习你不同样子的分布状态; 2.我要不断拉近和你的距离
D：通过两种方式，建立起一种，我要计算你是real image的一种概率（为了形成一个计算这种概率的方式，由于所有的数据是离散的，抽象的，我无法通过单纯的统计知识来进行学习，我这里没有模型，但是我可以通过网络的形式，来表达，量化出一个实体的operator，来实现一种定向功能的运算...），从两个角度出发 --> 第一个是R的我认可你是R，F的我判定你是fake;这样，我通过网络的不断逼近，来建立一个计算这种概率的网络通道
但是于此同时，为了让我这个fake图片变得更加真，也就是让我这个D处理fake当作real一样
G：认为D没有区分R和F
D：明确的区分R和F
这里所谓的概率分布没有特定制定是什么的概率分布，我只是以概率的方式来进行表达，概率是什么它是对一个东西出现频率的量化，我这里面确定是，这个大体上是属于谁的概率分布，并不管到底是什么的概率分布，怎么方便我进行对模型的利用，我就怎么定义，重要的是，我这个概率分布是要当real和fake同步的获得，我学习到的。，是根据我的输入格式，我的定义格式产生的一种概率分布;
这个GAN模型可以认为是一个学习，复制，量化的描述一个object的分布状态，模型的建立是为了利用finite的数据，进行一个分布特征的学习，一般来说，1000000的数据很大，但是在theory上依旧是finite的，theory上说的infinite的condition可能不时效，而且那种theory的正太分布布局有问题的特殊化，可能掉进没有考虑到的坑，但是通过数据的学习，我相当于构建一个模子，我不管你那里重要不重要，不管你这个factor有什么功能，我通过数据的模子，把你给扣下来，这就是数据建模的优势，而数据网络的建模，正是让这种思维的方式能够得到有效的实施，开辟了一个巨大的发展空间，无限种可能等待开发，通过网络构建出一个难以用函数来表达的处理过程
x是一个单位，我自己定义的，我为的就是学习出他的分布
（另外，所谓学习这个的概率分布，并不是，利用这个概率分布来推测，概率分布的概念应该是为了保持数学意义上，为了证明我存在收敛点，我的收敛过程是可行的，在finite capacity下是可以获得一个好的解
或者说，我这里的概率在迭代中只是一个对他的估测，我用minibatch也是为了对他进行一种估测，batch-size越多越好？不清楚，他和什么有着密切的联系
通过batch里面的均值进行估测data的分布，不过z的分布我们可以设定已知的，这个可以我们特定的写，不过一般均匀分布是一个好的选择）
我开始存在一个分布，一个有规律的分布，我需要建立一个模型，把每个符合这个均匀分布的set，映射到一个特定分布的set，这个映射过程我是通过，网络学习而获得的;
为了形成一种set对set的映射关系

Pz-->Pg我们想要Pg = Pd;
initial的概率分布对整个逼近过程的影响会有多大

本身还是一个单网络的问题，但是为了解决预测推断，以及半监督学习，而构建成的一个多网络，博弈性质的优化

输入的不一定是噪音可以是raw material，我可以训练一个把多种属性结合起来的算法

提取一个图像的特征，有效的读取信息
照片的混合：不同target，形成混合的属性，预知未来，男女混合，来预测生的宝宝
通过一个人生活的数据，来计算他的健康指数或者能够活的年数
通过外观的混合，来进行有效的预测，很多时候，利用这个方式来察觉到即将发生的事情，通过前兆的对比，来进行预测未来，预测最有可能发生的问题，比如避免911的事件
美剧：person of intreast -- > 巨大的机器通过实时监控，来预测最有可能发生的结果;

剖析：所有的数据可以看成两个量的组合： n1 × 标准的posture + n2 × distinct preference 可能随机性的体现出来
所以说我们可以学习到的分布应该可以包括：n1 × 标准分布 + n2 × distince preference distribution

所有的分布是我们定义的，只要保持data和z的一一对应关系，就可以做出一个很好的映射，来进行同步的拷贝
通过minibatch来学习preference的分布，利用D来不断的贴近
就是因为我没法准确的表示出来这个分布，所以我这边才利用D网络来换角度来看待问题
通过对D网络的训练，其实就是刻画一个检验是不是data分布的模型，用他来监督G网络调整一个接近data分布的g分布

D：刻画基本的分布，毕竟直接接触到data
mini-batch：为了刻画一个preference的分布，因为他接触到的是不同的data
可能：我定义E就是为了证明，或者建立一个基本的损失函数，证明它的最优解会导致Pdata = Pz，但是由于我们不知道这个Pdata，Pz的explicit的表达，就需要换种可行的方式，也就是说，我要找一个optimize A的过程中也在optimimize B，B得到一个结论，也可以认为是我optimaize A到的结论












通过访问大量的表层数据以及深度多层次的处理学习，以量变带动质变，将一个implict的关系搭成一个tangible的operator，作用于实时数据，完成场景到场景的时间片段的转换，复制历史，可能变得没有那么难以琢磨... gan有感... 我仿佛看到了 person of interest 里Finich创造的世界...
